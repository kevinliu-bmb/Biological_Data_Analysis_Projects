{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b321264",
   "metadata": {},
   "source": [
    "## Homework 09: The Return of The Ten Arcs\n",
    "##### By: Kevin Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3b25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logsumexp\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a584ec",
   "metadata": {},
   "source": [
    "### 1. Write a Simulator as a Positive Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8abe54",
   "metadata": {},
   "source": [
    "In this problem set, we are interested in assessing Lestrade's claim that both Arc2 and Arc3 are strongly on when it was previously thought that Arc2 and Arc3 are oppositely regulated.\n",
    "\n",
    "Before we begin our analysis, we will first utilize a generative probability model based on the works of Li and Dewey (2010) to generate positive control data in order to assess the performance of Lestrade's method. Specifically, we will simulate N = 1000000 observed read counts based on a simulated Arc locus structure with lengths $L_i$ and transcript abundances $\\tau$ that follows the problem set's specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce635dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tau_len(n_transcripts, min_len = 2, max_len = 4):\n",
    "    \"\"\"\n",
    "    Generates random transcript abundances and lengths between mix_len and max_len.\n",
    "    \"\"\"\n",
    "    tau_arr = np.random.dirichlet(np.ones(n_transcripts), size = 1)[0]\n",
    "    len_arr = np.random.randint(min_len, max_len+1, n_transcripts)\n",
    "    \n",
    "    return tau_arr, len_arr\n",
    "\n",
    "\n",
    "def mk_transcripts(len_arr):\n",
    "    \"\"\"\n",
    "    Generates a number of transcripts (i.e., Arc loci) based on a supplied list of transcript lengths \n",
    "    as a list of lists, where each nested list element represents a read.\n",
    "    \"\"\"\n",
    "    transcript_lst = []\n",
    "    n_segments = len(len_arr)\n",
    "    segment_lst = [i for i in range(n_segments)]\n",
    "    \n",
    "    for segment_start_pos, len_transcript in enumerate(len_arr):\n",
    "        segment_end_pos = segment_start_pos+len_transcript\n",
    "        if segment_end_pos > n_segments:\n",
    "            transcript = segment_lst[segment_start_pos:]+segment_lst[:segment_end_pos-n_segments]\n",
    "            transcript_lst.append(list(transcript))\n",
    "        else: \n",
    "            transcript_lst.append(segment_lst[segment_start_pos:segment_end_pos])\n",
    "    \n",
    "    return transcript_lst\n",
    "\n",
    "\n",
    "def tau_to_nu(tau_arr, len_arr):\n",
    "    \"\"\"\n",
    "    Converts transcript abundances to nucleotide abundances.\n",
    "    \"\"\"\n",
    "    nu_arr = tau_arr*len_arr\n",
    "    nu_arr = nu_arr/sum(nu_arr)\n",
    "    \n",
    "    return nu_arr\n",
    "\n",
    "\n",
    "def nu_to_tau(nu_arr, len_arr):\n",
    "    \"\"\"\n",
    "    Converts nucleotide abundances to transcript abundances.\n",
    "    \"\"\"    \n",
    "    tau_arr = nu_arr/len_arr\n",
    "    tau_arr = tau_arr/sum(tau_arr)\n",
    "    \n",
    "    return tau_arr\n",
    "\n",
    "\n",
    "def mk_reads(transcript_lst, tau_arr, len_arr, n_reads = 1000000):\n",
    "    \"\"\"\n",
    "    Samples n_reads number of reads from all possible transcripts based on the nucleotide abundances \n",
    "    and transcript lengths. \n",
    "    \"\"\"\n",
    "    n_transcripts = len(transcript_lst)\n",
    "    nu_arr = tau_to_nu(tau_arr, len_arr)\n",
    "    read_lst = []\n",
    "    for i in range(n_reads):\n",
    "        transcript_idx = np.random.choice(n_transcripts, p = nu_arr)\n",
    "        transcript = transcript_lst[transcript_idx]\n",
    "        segment = np.random.choice(transcript)\n",
    "        read_lst.append(segment)\n",
    "        \n",
    "    return read_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1abf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_arr, len_arr = mk_tau_len(n_transcripts = 10, min_len = 2, max_len = 4)\n",
    "transcript_lst = mk_transcripts(len_arr)\n",
    "read_lst = mk_reads(transcript_lst, tau_arr, len_arr, n_reads = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149045da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>$\\tau_i$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.099692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "      <td>0.128561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[3, 4, 5, 6]</td>\n",
       "      <td>0.005822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>0.049498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[5, 6, 7, 8]</td>\n",
       "      <td>0.216202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>[6, 7, 8, 9]</td>\n",
       "      <td>0.010340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>0.051502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>0.311506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>[9, 10, 1]</td>\n",
       "      <td>0.125515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>[10, 1, 2]</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length      Sequence  $\\tau_i$\n",
       "0       2        [1, 2]  0.099692\n",
       "1       3     [2, 3, 4]  0.128561\n",
       "2       4  [3, 4, 5, 6]  0.005822\n",
       "3       3     [4, 5, 6]  0.049498\n",
       "4       4  [5, 6, 7, 8]  0.216202\n",
       "5       4  [6, 7, 8, 9]  0.010340\n",
       "6       2        [7, 8]  0.051502\n",
       "7       2        [8, 9]  0.311506\n",
       "8       3    [9, 10, 1]  0.125515\n",
       "9       3    [10, 1, 2]  0.001362"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Length': len_arr, \n",
    "              'Sequence': [[j+1 for j in i] for i in transcript_lst], \n",
    "              '$\\tau_i$': tau_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0305b40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Read_Sequence</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>81775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>82848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>48485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>66701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>97708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>101595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>100631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>212843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>161485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>45929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Read_Sequence  Counts\n",
       "9              1   81775\n",
       "3              2   82848\n",
       "0              3   48485\n",
       "4              4   66701\n",
       "2              5   97708\n",
       "8              6  101595\n",
       "6              7  100631\n",
       "1              8  212843\n",
       "7              9  161485\n",
       "5             10   45929"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(Counter(read_lst), orient = \"index\").reset_index()\n",
    "df.columns = [\"Read_Sequence\", \"Counts\"]\n",
    "df.Read_Sequence = df.Read_Sequence+1\n",
    "sorted_df = df.sort_values(\"Read_Sequence\")\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44ed3b",
   "metadata": {},
   "source": [
    "### 2. Calculate the Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab634d2",
   "metadata": {},
   "source": [
    "Since we have generated simulated positive control data with known transcript abudnances $\\tau$ and lengths $L_i$, we will begin to assess Lestrade's method in estimating the parameters by comparing it to our known true parameters through calculating the log-likelihoods. To implement this for a given Arc locus structure, we calculate the log probability of the observed read counts $r_k$ given that the model and parameters $\\tau$ and $L$ are known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92e1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(read_lst, transcript_lst, tau_arr, len_arr):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood of transcript abundances.\n",
    "    \"\"\"\n",
    "    nu_arr = tau_to_nu(tau_arr, len_arr)\n",
    "    log_prob_lst = []\n",
    "    unique_read_arr, unique_read_count_arr = np.unique(read_lst, return_counts = True)\n",
    "    \n",
    "    for read_idx, read in enumerate(unique_read_arr):\n",
    "        read_prob_lst = []\n",
    "        for transcript_idx, transcript in enumerate(transcript_lst):\n",
    "            if read in transcript:\n",
    "                read_prob_lst.append(np.log(nu_arr[transcript_idx]/len_arr[transcript_idx]))\n",
    "        \n",
    "        log_prob_lst.append(logsumexp(read_prob_lst)*unique_read_count_arr[read_idx])\n",
    "        \n",
    "    nll = -np.sum(log_prob_lst)\n",
    "    \n",
    "    return nll\n",
    "\n",
    "\n",
    "def get_les_tau(r, T, L):\n",
    "    \"\"\"\n",
    "    Adapted from w09-naive.py.\n",
    "    Generates transcript abundance estimates using Lestrade's method.\n",
    "    \"\"\"\n",
    "    S = T    # S = R = T : there are T transcripts (Arc1..Arc10), S segments (A..J), R reads (a..j)\n",
    "\n",
    "    # Count how often each segment A..J is used in the isoforms i\n",
    "    # We'll use that to split observed read counts across the isoforms\n",
    "    # that they might have come from.\n",
    "    #\n",
    "    segusage = np.zeros(S).astype(int)\n",
    "    for i in range(T):\n",
    "        for j in range(i,i+L[i]): \n",
    "            segusage[j%S] += 1\n",
    "\n",
    "    # Naive analysis:\n",
    "    #\n",
    "    c  = np.zeros(T)\n",
    "    for i in range(T):\n",
    "        for k in range(i,i+L[i]):\n",
    "            c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                                  # and assign 1/usage count to each transcript\n",
    "                                                                  # that contains segment j.\n",
    "    Z       = np.sum(c)\n",
    "    est_nu  = np.divide(c, Z)       # nucleotide abundance\n",
    "    est_tau = np.divide(est_nu, L)  # convert to TPM, transcript abundance\n",
    "    est_tau = np.divide(est_tau, np.sum(est_tau))\n",
    "    \n",
    "    return est_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74914735",
   "metadata": {},
   "outputs": [],
   "source": [
    "les_tau_arr = get_les_tau(sorted_df[\"Counts\"], len(transcript_lst), len_arr)\n",
    "true_nll = nll(read_lst, transcript_lst, tau_arr, len_arr)\n",
    "les_nll = nll(read_lst, transcript_lst, les_tau_arr, len_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0921b20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True $\\tau_i$</th>\n",
       "      <th>Lestrade's $\\hat\\tau_i$</th>\n",
       "      <th>$\\Delta\\tau_i$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099692</td>\n",
       "      <td>0.126446</td>\n",
       "      <td>-0.026754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.128561</td>\n",
       "      <td>0.142659</td>\n",
       "      <td>-0.014098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.079972</td>\n",
       "      <td>-0.074150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049498</td>\n",
       "      <td>0.059352</td>\n",
       "      <td>-0.009853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.216202</td>\n",
       "      <td>0.078117</td>\n",
       "      <td>0.138085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.086559</td>\n",
       "      <td>-0.076219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.051502</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>-0.055791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.311506</td>\n",
       "      <td>0.107759</td>\n",
       "      <td>0.203747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.125515</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.037536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.123865</td>\n",
       "      <td>-0.122503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True $\\tau_i$  Lestrade's $\\hat\\tau_i$  $\\Delta\\tau_i$\n",
       "0       0.099692                 0.126446       -0.026754\n",
       "1       0.128561                 0.142659       -0.014098\n",
       "2       0.005822                 0.079972       -0.074150\n",
       "3       0.049498                 0.059352       -0.009853\n",
       "4       0.216202                 0.078117        0.138085\n",
       "5       0.010340                 0.086559       -0.076219\n",
       "6       0.051502                 0.107293       -0.055791\n",
       "7       0.311506                 0.107759        0.203747\n",
       "8       0.125515                 0.087979        0.037536\n",
       "9       0.001362                 0.123865       -0.122503"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"True $\\tau_i$\": tau_arr,\n",
    "              \"Lestrade's $\\hat\\tau_i$\": les_tau_arr,\n",
    "              \"$\\Delta\\tau_i$\": tau_arr-les_tau_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef64d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log-likelihood of true parameters: 2194330.836\n",
      "Negative log-likelihood of Lestrade's parameters: 2271788.715\n",
      "Difference between negative log-likelihoods: 77457.879\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative log-likelihood of true parameters: \" + str(np.round(true_nll, 3)))\n",
    "print(\"Negative log-likelihood of Lestrade's parameters: \" + str(np.round(les_nll, 3)))\n",
    "print(\"Difference between negative log-likelihoods: \" + str(np.abs(np.round(true_nll-les_nll, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77eec7",
   "metadata": {},
   "source": [
    "Based on the above table comparing the true transcript abundances to Lestrade's transcript abundances, it is apparent that Lestrade's method of estimating transcript abundances are different from our positive control true transcript abudnances. This difference is reinforced by the calculated difference in negative log-likelihoods, where the negative log-likelihood of the true parameters is significantly larger than that of Lestrade's parameters, suggesting that Lestrade's estimates are significantly less likely than the true parameters.\n",
    "\n",
    "We will attempt to optimize the negative log-likelihoods by using an expectation maximization (EM) algorithm to obtain better parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3d3fb",
   "metadata": {},
   "source": [
    "### 3. Estimate Isoform Abundances by EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68840d",
   "metadata": {},
   "source": [
    "Here, we will write an EM algorithm to estimate the unknown isoform abundances $\\tau_i$ for each isoform Arc1, ..., Arc10, given read counts $r_k$ and the structure of the Arc locus including the lengths $L_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa76dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Adapted from w09-naive.py.\n",
    "    Reads in a data table and returns the read counts and transcript lengths.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        #   The first line is \"The transcripts of the sand mouse Arc locus\"\n",
    "        line  = f.readline()\n",
    "        match = re.search(r'^The (\\d+) transcripts', line)\n",
    "        T     = int(match.group(1))\n",
    "\n",
    "        # The next T lines are \n",
    "        #       \n",
    "        # tau's may be present, or obscured (\"xxxxx\")\n",
    "        tau       = np.zeros(T)\n",
    "        L         = np.zeros(T).astype(int)\n",
    "        tau_known = True   # until we see otherwise\n",
    "        for i in range(T):\n",
    "            fields    = f.readline().split()\n",
    "            if fields[1] == \"xxxxx\":\n",
    "                tau_known = False\n",
    "            else:\n",
    "                tau[i] = float(fields[1])\n",
    "            L[i]      = int(fields[2])\n",
    "\n",
    "        # after a blank line,\n",
    "        # 'The  read sequences':\n",
    "        line  = f.readline()\n",
    "        line  = f.readline()\n",
    "        match = re.search(r'The (\\d+) read sequences', line)\n",
    "        N     = int(match.group(1))\n",
    "\n",
    "        # the next T lines are \n",
    "        #\n",
    "        r = np.zeros(T).astype(int)\n",
    "        for k in range(T):\n",
    "            fields = f.readline().split()\n",
    "            r[k]   = fields[1]\n",
    "    \n",
    "    return r, L\n",
    "    \n",
    "\n",
    "def EM(read_lst, transcript_lst, len_arr, threshold = 0.001):\n",
    "    \"\"\"\n",
    "    Perform EM optimization with a maximum of n_iterations given parameters.\n",
    "    \"\"\"\n",
    "    tau_arr = mk_tau_len(n_transcripts = len(len_arr), min_len = 2, max_len = 4)[0]\n",
    "    nu_arr = nu_to_tau(tau_arr, len_arr)\n",
    "    unique_read_arr, unique_read_count_arr = np.unique(read_lst, return_counts = True)\n",
    "\n",
    "    nll_new = 0\n",
    "    nll_old = nll(read_lst, transcript_lst, tau_arr, len_arr)\n",
    "    nll_lst = [nll_old]\n",
    "\n",
    "    while np.abs(nll_new - nll_old) > threshold:\n",
    "        nll_old = nll(read_lst, transcript_lst, tau_arr, len_arr)\n",
    "        count_arr = np.zeros(10)\n",
    "\n",
    "        for read_idx, read in enumerate(unique_read_arr):\n",
    "            read_prob_lst = []\n",
    "            read_idx_lst = []\n",
    "            for transcript_idx, transcript in enumerate(transcript_lst):\n",
    "                if read in transcript:\n",
    "                    read_idx_lst.append(transcript_idx)\n",
    "                    num = np.log(nu_arr[transcript_idx]/len_arr[transcript_idx])\n",
    "                    read_prob_lst.append(num)\n",
    "\n",
    "            read_prob_lst -= logsumexp(read_prob_lst)\n",
    "            np.add.at(count_arr, read_idx_lst, np.exp(read_prob_lst)*unique_read_count_arr[read_idx])\n",
    "\n",
    "        nu_arr = count_arr/np.sum(count_arr)\n",
    "        tau_arr = nu_to_tau(nu_arr, len_arr)\n",
    "        nll_new = nll(read_lst, transcript_lst, tau_arr, len_arr)\n",
    "        nll_lst.append(nll_new)\n",
    "\n",
    "    print(\"Number of iterations to convergence: \" + str(len(nll_lst)-1))\n",
    "    \n",
    "    return tau_arr, nll_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a6e50",
   "metadata": {},
   "source": [
    "We will first run our EM algorithm, calculate the negative log-likelihood of the estimated parameters, and compare it to that of the true parameters from our simulated positive control data to assess our EM algorithm performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f750a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convergence: 3553\n"
     ]
    }
   ],
   "source": [
    "em_tau_arr, em_nll_lst = EM(read_lst, transcript_lst, len_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915066a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EM $\\hat\\tau_i$</th>\n",
       "      <th>True $\\hat\\tau_i$</th>\n",
       "      <th>$\\Delta\\hat\\tau_i$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099160</td>\n",
       "      <td>0.099692</td>\n",
       "      <td>-0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124728</td>\n",
       "      <td>0.128561</td>\n",
       "      <td>-0.003832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.003569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050273</td>\n",
       "      <td>0.049498</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.209904</td>\n",
       "      <td>0.216202</td>\n",
       "      <td>-0.006299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.001448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.051502</td>\n",
       "      <td>0.004502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.311683</td>\n",
       "      <td>0.311506</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.122136</td>\n",
       "      <td>0.125515</td>\n",
       "      <td>-0.003380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.003574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EM $\\hat\\tau_i$  True $\\hat\\tau_i$  $\\Delta\\hat\\tau_i$\n",
       "0         0.099160           0.099692           -0.000532\n",
       "1         0.124728           0.128561           -0.003832\n",
       "2         0.009391           0.005822            0.003569\n",
       "3         0.050273           0.049498            0.000774\n",
       "4         0.209904           0.216202           -0.006299\n",
       "5         0.011788           0.010340            0.001448\n",
       "6         0.056003           0.051502            0.004502\n",
       "7         0.311683           0.311506            0.000176\n",
       "8         0.122136           0.125515           -0.003380\n",
       "9         0.004936           0.001362            0.003574"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"EM $\\hat\\tau_i$\": em_tau_arr,\n",
    "              \"True $\\hat\\tau_i$\": tau_arr,\n",
    "              \"$\\Delta\\hat\\tau_i$\": em_tau_arr-tau_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4f4c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log-likelihood of true parameters: 2194330.836\n",
      "Negative log-likelihood of EM parameters: 2194330.094\n",
      "Difference between negative log-likelihoods: 0.742\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative log-likelihood of true parameters: \" + str(np.round(true_nll, 3)))\n",
    "print(\"Negative log-likelihood of EM parameters: \" + str(np.round(em_nll_lst[-1], 3)))\n",
    "print(\"Difference between negative log-likelihoods: \" + str(np.abs(np.round(true_nll-em_nll_lst[-1], 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa7d19",
   "metadata": {},
   "source": [
    "Based on the results shown above, we observe that there is a marginal difference in the negative log-likelihoods between the true parameters and the EM parameters, suggesting that our EM algorithm is both accurate and robust in estimating the unknown isoform abundances $\\tau_i$ using the simulated positive control data.\n",
    "\n",
    "Subsequently, we will read in the supplementary data from Lestrade et al., compare our EM algorithm's performance to that of Lestrade's method, and examine Lestrade's claim that Arc2 and Arc3 are both strongly on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2de3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convergence: 1544\n"
     ]
    }
   ],
   "source": [
    "r, L = read_data(\"w09-data.out\")\n",
    "data_read_lst = []\n",
    "\n",
    "for read_idx, read_count in enumerate(r):\n",
    "    data_read_lst += [read_idx]*read_count\n",
    "    \n",
    "data_transcript_lst = mk_transcripts(L)\n",
    "\n",
    "data_em_tau_arr, data_em_nll_lst = EM(data_read_lst, data_transcript_lst, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c78ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_les_tau_arr = get_les_tau(r, len(data_transcript_lst), L)\n",
    "data_em_nll = nll(data_read_lst, data_transcript_lst, data_em_tau_arr, L)\n",
    "data_les_nll = nll(data_read_lst, data_transcript_lst, data_les_tau_arr, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd0eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isoform</th>\n",
       "      <th>Lestrade's $\\hat\\tau_i$</th>\n",
       "      <th>EM $\\hat\\tau_i$</th>\n",
       "      <th>$\\Delta\\hat\\tau_i$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arc1</td>\n",
       "      <td>0.165987</td>\n",
       "      <td>0.147305</td>\n",
       "      <td>0.018683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arc2</td>\n",
       "      <td>0.248437</td>\n",
       "      <td>0.497552</td>\n",
       "      <td>-0.249115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arc3</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.112993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arc4</td>\n",
       "      <td>0.044097</td>\n",
       "      <td>0.049636</td>\n",
       "      <td>-0.005539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arc5</td>\n",
       "      <td>0.039440</td>\n",
       "      <td>0.036410</td>\n",
       "      <td>0.003030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arc6</td>\n",
       "      <td>0.041721</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.022101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arc7</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.036389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arc8</td>\n",
       "      <td>0.079994</td>\n",
       "      <td>0.135416</td>\n",
       "      <td>-0.055422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arc9</td>\n",
       "      <td>0.085655</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>0.031848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arc10</td>\n",
       "      <td>0.136573</td>\n",
       "      <td>0.051542</td>\n",
       "      <td>0.085031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Isoform  Lestrade's $\\hat\\tau_i$  EM $\\hat\\tau_i$  $\\Delta\\hat\\tau_i$\n",
       "0    Arc1                 0.165987         0.147305            0.018683\n",
       "1    Arc2                 0.248437         0.497552           -0.249115\n",
       "2    Arc3                 0.115051         0.002057            0.112993\n",
       "3    Arc4                 0.044097         0.049636           -0.005539\n",
       "4    Arc5                 0.039440         0.036410            0.003030\n",
       "5    Arc6                 0.041721         0.019620            0.022101\n",
       "6    Arc7                 0.043045         0.006656            0.036389\n",
       "7    Arc8                 0.079994         0.135416           -0.055422\n",
       "8    Arc9                 0.085655         0.053807            0.031848\n",
       "9   Arc10                 0.136573         0.051542            0.085031"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_em_df = pd.DataFrame({\"Isoform\": [\"Arc\" + str(i+1) for i in range(len(L))],\n",
    "                           \"Lestrade's $\\hat\\tau_i$\": data_les_tau_arr,\n",
    "                           \"EM $\\hat\\tau_i$\": data_em_tau_arr,\n",
    "                           \"$\\Delta\\hat\\tau_i$\": data_les_tau_arr-data_em_tau_arr})\n",
    "data_em_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7913e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log-likelihood of EM parameters: 2021934.129\n",
      "Negative log-likelihood of Lestrade's parameters: 2084370.855\n",
      "Difference between negative log-likelihoods: 62436.726\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative log-likelihood of EM parameters: \" + str(np.round(data_em_nll, 3)))\n",
    "print(\"Negative log-likelihood of Lestrade's parameters: \" + str(np.round(data_les_nll, 3)))\n",
    "print(\"Difference between negative log-likelihoods: \" + str(np.abs(np.round(data_em_nll-data_les_nll, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d65d23",
   "metadata": {},
   "source": [
    "As shown from the above results, our EM algorithm is superior to that of Lestrade's approach given the large difference in the negative log-likelihoods, where our EM estimated parameters yielded negative log-likelihoods that are much larger than that of Lestrade's estimated parameters. Furthermore, we see from the $\\Delta\\hat\\tau_i$ in the table of estimated transcript abundances that Lestrade's method underestimates Arc2 and overestimates Arc3 relative to our EM algorithm, which might explain his unlikely claim that Arc2 and Arc3 are both strongly on.\n",
    "\n",
    "Next, we will further examine Lestrade's claim by extracting the top two isoforms that have the largest and smallest transcript abundances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d959e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most abundant transcripts are Arc2 and Arc1, which accounts for 64.5% of the population together.\n",
      "The two least abundant transcripts are Arc7 and Arc3, which accounts for 0.871% of the population together.\n"
     ]
    }
   ],
   "source": [
    "sorted_data_em_df = data_em_df.sort_values(\"EM $\\hat\\tau_i$\", ascending = False)\n",
    "\n",
    "max2_tau_df = sorted_data_em_df.head(2)\n",
    "min2_tau_df = sorted_data_em_df.tail(2)\n",
    "\n",
    "print(\"The two most abundant transcripts are \" + \n",
    "      str(' and '.join([str(i) for i in list(max2_tau_df[\"Isoform\"])])) + \n",
    "      \", which accounts for \" + str(round(max2_tau_df[\"EM $\\hat\\tau_i$\"].sum()*100, 1)) + \n",
    "      \"% of the population together.\")\n",
    "\n",
    "print(\"The two least abundant transcripts are \" + \n",
    "      str(' and '.join([str(i) for i in list(min2_tau_df[\"Isoform\"])])) + \n",
    "      \", which accounts for \" + str(round(min2_tau_df[\"EM $\\hat\\tau_i$\"].sum()*100, 3)) + \n",
    "      \"% of the population together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488637ff",
   "metadata": {},
   "source": [
    "With our previous findings in mind, we further observe that Arc2 is one of the most abundant transcripts and that Arc3 is one of the least abundant transcripts, based on such evidence we can conclude that Lestrade's claim that Arc2 and Arc3 are both strongly on is false.\n",
    "\n",
    "Lestrade's method is expectedly poor performing as we notice that his method uniformly assigns read counts to the respective isoforms in an arbitrary manner, which is analogous to running a single iteration of our EM algorithm with the initial parameters. Therefore, it is unsurprising that our iterative EM optimization algorithm outperforms Lesetrade's method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
